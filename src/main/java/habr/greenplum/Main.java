/**
 *
 *                                      https://habr.com/ru/company/tinkoff/blog/267733/
 * GreenPlum - реляционная СУБД, имеющая массово-параллельную (massive parallel processing) архитектуру без разделения
 * ресурсов (Shared Nothing). В GP может быть реализовано как колоночное, так и строковое хранение данных.
 * Основные термины:
 * Master instance - он же просто мастер. Инстанс Postgres, являющийся одновременно координатором и входной точкой для
 *      пользователя в кластере.
 * Master host - он же сервер-мастер. Сервер, на котором работает Master instance.
 * Secondary master instance - инстанс Postgres, являющийся резервным мастером, включается в работу в случае недоступности
 *      основного мастера (переключение происходит вручную).
 * Primary segment instance - или просто сегмент. Инстанс Postgres, являющийся одним из сегментов. Именно сегменты
 *      непосредственно хранят данные, выполняют с ними операции и отдают результаты мастеру (в общем случае). По сути,
 *      сегмент - это самый обычный инстанс PostgreSQL с настроенной WAL-репликацией в свое зеркало на другом сервере.
 * Mirror segment instance - оно же зеркало. Инстанс Postgres, являющийся зеркалом одного из primary-сегментов.
 *      Автоматически принимает на себя роль primary в случае падения оного. GP поддерживает только 1-to-1 репликацию
 *      сегментов: для каждого primary может быть только один mirror.
 * Segment host - сервер-сегмент. Сервер, на котором работает один или несколько сегментов и/или зеркал.
 * В общем случае, кластер GP состоит из нескольких серверов-сегментов, одного сервера-мастера и одного
 * сервера-секондари-мастера, соединенных между собой одной или несколькими обособленными сетями (1.png). При выборе
 * числа серверов-сегментов важно правильно выбрать соотношение кластера "число процессоров/Тб данных". Чем больше ядер
 * процессора приходится на единицу данных - тем лучше. При выборе числа сегментов в кластере (которое, в общем случае,
 * к числу серверов никак не привязано) необходимо помнить следующее:
 *  - все ресурсы сервера делятся между всеми сегментами на сервере(нагрузкой зеркал, если они на этих же серверах,
 *      можно условно пренебречь)
 *  - каждый запрос на одном сегменте не может потреблять процессорных ресурсов больше, чем одно ядро CPU. Что это
 *      значит? Допустим на нашем сервере 32 ядра. На нем же создано 4 сегмента GP. То есть на каждый сегмент приходится
 *      по 8 ядер. Этот сервер обрабатывает, в среднем, 4 запроса одновременно. То есть, каждый сегмент использует для
 *      обработки запросов 4 ядра (по ядру на каждый запрос). Остальные ядра простаивают. Таким образом, для улучшения
 *      производительности, нам нужно не 4 сегмента, а 8.
 *  - шататный процесс бекапа и рестора данных "из коробки" работает только на кластерах, имеющих одинаковое количество
 *      сегментов. Восстановить данные, забекапленные на кластере из 96 сегментов в кластер из 100 сегментов просто так
 *      не получится.
 * Хранение данных.
 * В GP реализуется классическая схема разделения данных. Каждая таблица делится на количество сегментов N. Таким образом,
 * на каждом сегменте хранится 1/N строк таблицы. Логика разбиения таблицы на сегменты задается ключем дистрибуции -
 * специальным полем, на основе которого любую строку можно однозначно отнести к одному из сегментов.
 * Ключ (поле или набор полей) дистрибуции - очень важное понятие в GP. В свете сказанного выше - логично предположить,
 * что GreenPlum работает со скоростью самого медленного сегмента, это означает, что любой перекос в количестве данных
 * между сегментами ведет к деградации производительности кластера. ВАЖНО, что GP не поддерживает UPDATE поля, по
 * которому распределена таблица. Поле для дистрибуции можно менять в уже созданной таблице, однако после этого, ее нужно
 * перераспределить. Именно по полю дистрибуции GP совершает самые оптимальные JOIN-ы: в случае, если в обоих таблицах
 * поля, по которым совершается JOIN, являются ключами дистрибуции, JOIN выполняется локально на сегменте. Если же это
 * условие не выполнено, то GP придется или перераспределить обе таблицы по искомому полю, или закинуть одну из таблиц
 * целиком на каждый сегмент (операция BROADCAST) и уже затем джойнить таблицы локально на сегменте.
 * Взаимодействие с клиентами.
 * В общем случае, все взаимодейтсвие клиентов с кластером ведется только через мастер - именно он отвечает клиентам,
 * выдает им результат запроса и т д. Обычные клиенты-пользователи не имеют сетевого ддоступа к серверам-сегментам.
 * Небольшие заметки.
 * GP - аналитическая БД, предназначенная для небольшого числа одновременных запросов, выполняющих тяжелые операции над
 * большим объемом данных. Большое число (более 600 запросов в секунду) легких запросов/транзакций, выполняющих одну
 * операцию, негативно сказывается на производительности базы из-за ее распределенной архитектуры - каждая транзакция
 * на мастере порождает N транзакций на сегментах.
 *
 * */

package habr.greenplum;

public class Main {
}
